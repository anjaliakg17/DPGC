train.py gap-inf --dataset facebook --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 4 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100 
train.py sage-inf --dataset facebook --base_layers 2 --head_layers 1 --mp_layers 2 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-edp --dataset facebook --epsilon 4 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 2 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100
train.py sage-edp --dataset facebook --epsilon 4 --base_layers 2 --head_layers 1 --mp_layers 1 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py mlp --dataset facebook --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-ndp --dataset facebook --epsilon 8 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 2 --max_degree 100 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 256 --encoder_epochs 10
train.py sage-ndp --dataset facebook --epsilon 8 --base_layers 2 --head_layers 1 --max_degree 100 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 256
train.py mlp-dp --dataset facebook --epsilon 8 --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 256
train.py gap-inf --dataset reddit --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 4 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100
train.py sage-inf --dataset reddit --base_layers 2 --head_layers 1 --mp_layers 4 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-edp --dataset reddit --epsilon 4 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 2 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100
train.py sage-edp --dataset reddit --epsilon 4 --base_layers 2 --head_layers 1 --mp_layers 1 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py mlp --dataset reddit --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-ndp --dataset reddit --epsilon 8 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 3 --max_degree 400 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 2048 --encoder_epochs 10
train.py sage-ndp --dataset reddit --epsilon 8 --base_layers 2 --head_layers 1 --max_degree 100 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 2048
train.py mlp-dp --dataset reddit --epsilon 8 --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 2048
train.py gap-inf --dataset amazon --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 5 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100
train.py sage-inf --dataset amazon --base_layers 2 --head_layers 1 --mp_layers 4 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-edp --dataset amazon --epsilon 4 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 5 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full --encoder_epochs 100
train.py sage-edp --dataset amazon --epsilon 4 --base_layers 2 --head_layers 1 --mp_layers 1 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py mlp --dataset amazon --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --batch_norm True --epochs 100 --batch_size full
train.py gap-ndp --dataset amazon --epsilon 8 --encoder_layers 2 --base_layers 1 --head_layers 1 --combine cat --hops 2 --max_degree 100 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 4096 --encoder_epochs 10
train.py sage-ndp --dataset amazon --epsilon 8 --base_layers 2 --head_layers 1 --max_degree 100 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 4096
train.py mlp-dp --dataset amazon --epsilon 8 --num_layers 3 --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --max_grad_norm 1 --epochs 10 --batch_size 4096
